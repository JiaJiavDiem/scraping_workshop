{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun  9 17:09:58 2019\n",
    "\n",
    "@author: chris\n",
    "\"\"\"\n",
    "\n",
    "# This file contains the names and URL of companies to scrape\n",
    "# Start: emtpy file\n",
    "# End: companies in SQLite database\n",
    "\n",
    "# Step1D: Additional characteristics can be added to the company info\n",
    "\n",
    "# code taken from http://www.sqlitetutorial.net/sqlite-python/creating-database/\n",
    "import sqlite3\n",
    "from scraper import create_connection, create_table, modify_table, read_CSV, write_to_db, retrieve_data, write_to_db_scrape1, scrape\n",
    "import bs4\n",
    "\n",
    "database = \"scrape.db\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1A: Set up a SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "        # define the database        \n",
    "        global database                \n",
    "        # create a database connection\n",
    "        conn=create_connection(database)\n",
    "        conn.close()\n",
    "        print(\"done\")\n",
    "\n",
    "create_database()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1B: Set up tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_companies_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS companies(\n",
    "        id integer PRIMARY KEY,\n",
    "        Company text NOT NULL,\n",
    "        Website text,\n",
    "        CountryOfIncorporation text,\n",
    "        MarketDomain text        \n",
    "        );\"\"\"\n",
    "\n",
    "sql_create_1stscrape_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS firstScrape(\n",
    "        Company text NOT NULL,\n",
    "        Website text,\n",
    "        Scraped_text text,\n",
    "        Links_1st text,\n",
    "        Company_id integer NOT NULL\n",
    "        );\"\"\"\n",
    "\n",
    "[\"Company\", \"ID_link\", \"URL\", \"Source_code\", \"Text\"]\n",
    "\n",
    "sql_create_2ndscrape_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS secondScrape(\n",
    "        Company text NOT NULL,\n",
    "        ID_link integer NOT NULL,\n",
    "        URL text,\n",
    "        Source_code text,\n",
    "        Text text\n",
    "        );\"\"\"\n",
    "\n",
    "sql_create_topics = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS topics(\n",
    "        index_topic int,\n",
    "        topic text\n",
    "        );\"\"\"\n",
    "\n",
    "def create_tables():\n",
    "        global database\n",
    "                        \n",
    "        # create a database connection\n",
    "        conn = create_connection(database)\n",
    "\n",
    "        if conn is not None:\n",
    "            # create project table\n",
    "            create_table(conn, sql_create_companies_table)\n",
    "            create_table(conn, sql_create_1stscrape_table)\n",
    "            create_table(conn, sql_create_2ndscrape_table)\n",
    "#             create_table(conn, sql_create_topics)\n",
    "        \n",
    "            num_topics_int = 10\n",
    "\n",
    "            for num in range(num_topics_int):\n",
    "                name = \"Score_\" + str(num)\n",
    "                modify_table(conn, (\"ALTER TABLE secondScrape ADD \" + name + \" integer\"))\n",
    "                        \n",
    "            print(\"all succeeded\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(\" error! cannot create the database connection\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1C: Reading the CSV file and writing to the SQLite file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CSV = read_CSV(\"191005_PilotGroup.csv\")\n",
    "table = \"companies\"\n",
    "conn = create_connection(database)\n",
    "write_to_db(df_CSV, table, conn)\n",
    "conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sites():\n",
    "    table = \"companies\"\n",
    "    sql = \"SELECT * FROM \" + table\n",
    "    \n",
    "    conn = create_connection(database)\n",
    "    # Step 2A\n",
    "    df_data = retrieve_data(sql, conn)\n",
    "    \n",
    "    # Step 2B\n",
    "    for index, row in df_data.iterrows():\n",
    "        id_link = index\n",
    "        company = row['Company']\n",
    "        url = row['Website']\n",
    "        try:\n",
    "            res_text = scrape(url)\n",
    "        except:\n",
    "            res_text = \"No response\"\n",
    "                \n",
    "        # Step 2C\n",
    "        soup = bs4.BeautifulSoup(res_text, 'lxml')\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            print(link['href'])\n",
    "            links.append(link['href'])\n",
    "        links_1st = []\n",
    "        for link in links:\n",
    "            if \"https\" in link and not url in link:\n",
    "                next\n",
    "            elif link == url:\n",
    "                next\n",
    "            else:\n",
    "                links_1st.append(link)\n",
    "        links_1st = str(list(set(links_1st)))\n",
    "        links_1st = links_1st.replace(\"'\", \"\")\n",
    "        links_1st = links_1st.replace('\"', \"\")\n",
    "        links_1st = links_1st.replace(\" \", \"\")  \n",
    "        write_to_db_scrape1(company, url, res_text, links_1st, id_link, conn)    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_sites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
